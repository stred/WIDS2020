{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIDS Week 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JzVyKZiSEZHO_TacrHKiYzOtkrpB8bal",
      "authorship_tag": "ABX9TyP57DA6q6U3beWMh5Qu4c4B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbYaSoz-E7Mv",
        "colab_type": "text"
      },
      "source": [
        "# Women in Data Science\n",
        "## Week 3 - Model Selection\n",
        "\n",
        "### Stephen Redmond\n",
        "Enterprise Insight Studio Lead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_6xr2A-BoWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect to my Google Drive\n",
        "from google.colab import drive\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_UPKRg-BtOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive/WIDS'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUzNgwkgFhlC",
        "colab_type": "text"
      },
      "source": [
        "#Loading the DataFrame from a CSV file using pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzuDEVXSCV27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the pandas library\n",
        "# Traditionally, this is loaded and named as \"pd\"\n",
        "import pandas as pd\n",
        "# The numpy library is traditionally \"np\"\n",
        "import numpy as np\n",
        "\n",
        "# Titanic data from https://www.kaggle.com/c/titanic/data\n",
        "# Load the Titanic data file from my Google Drive\n",
        "df = pd.read_csv('/content/drive/My Drive/WIDS/titanic/train.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb76Ue5wf79N",
        "colab_type": "text"
      },
      "source": [
        "# Data Dictionary\n",
        "| Variable | Definition                                 | Key                                            |\n",
        "|----------|--------------------------------------------|------------------------------------------------|\n",
        "| survival | Survival                                   | 0 = No, 1 = Yes                                |\n",
        "| pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n",
        "| sex      | Sex                                        |                                                |\n",
        "| Age      | Age in years                               |                                                |\n",
        "| sibsp    | # of siblings / spouses aboard the Titanic |                                                |\n",
        "| parch    | # of parents / children aboard the Titanic |                                                |\n",
        "| ticket   | Ticket number                              |                                                |\n",
        "| fare     | Passenger fare                             |                                                |\n",
        "| cabin    | Cabin number                               |                                                |\n",
        "| embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNlXy0tcYk2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Have a quick look\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkmMtvlxh0wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And the non-numeric fields\n",
        "df[[\"Name\",\"Sex\",\"Ticket\",\"Cabin\",\"Embarked\"]].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoyBBHMljoHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some of these fields are less than useful:\n",
        "# - Cabin has many missing values\n",
        "# - Name, Ticket and PassengerId have too many unique values\n",
        "df = df.drop(columns = ['Cabin','Name','Ticket','PassengerId'])\n",
        "\n",
        "# The Embarked field has 2 missing - let's just assume it was Southampton\n",
        "df[\"Embarked\"].fillna(\"S\", inplace=True)\n",
        "\n",
        "# Age has NaN values ... what should we do?\n",
        "df['Age'].fillna(df['Age'].mode()[0], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xUw5InL_FGn0"
      },
      "source": [
        "# More fun with scikit-learn\n",
        "We previously looked at some useful initial uses of sklearn, like splitting a dataset into training and test.\n",
        "\n",
        "Let's play with more models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcr0ahC5neNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new feature called FamilySize\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] \n",
        "df['IsAlone'] = 0\n",
        "df.loc[df['FamilySize'] == 0, 'IsAlone'] = 1\n",
        "\n",
        "# Splitting into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X = train_test_split(df, test_size = 0.2) \n",
        "\n",
        "# Train my models with a small # of features\n",
        "from sklearn import tree\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "\n",
        "X = pd.get_dummies(train_X[[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"FamilySize\"]],drop_first=True) \n",
        "y = train_X[\"Survived\"]\n",
        "dt = dt.fit(X, y)\n",
        "X_test = pd.get_dummies(test_X[[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"FamilySize\"]],drop_first=True) #.to_numpy()\n",
        "y_test = test_X[\"Survived\"]\n",
        "y_pred = dt.predict(X_test)\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQMmuc60qj3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calcualte F1 score: F1 = 2 * (precision * recall) / (precision + recall)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ULCOyQypfZA",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest classifier\n",
        "\"Ensemble\" method - builds many decision trees and will return the best value from all of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7hSTCl1HrEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf=RandomForestClassifier()\n",
        "rf.fit(X=X,y=y)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhgWquRPqAr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PLhCjdhD1OE",
        "colab_type": "text"
      },
      "source": [
        "# Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxQakW09D6pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need to scale our data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit our scaler to the OneHot encoded data and transform it\n",
        "X = scaler.fit_transform(pd.get_dummies(train_X[[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"FamilySize\"]],drop_first=True)) \n",
        "y = train_X[\"Survived\"]\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8GrjB4BEfPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95pBlJqCE3K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"R^2: \" + str(reg.score(X, y)) + \"\\nCoefficients: \" + str(reg.coef_) + \"\\nIntercept:\" + str(reg.intercept_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pQsFXbuFk4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = reg.predict(X_test_scaled)\n",
        "\n",
        "# Confusion Matrix\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9-ry2LAF63x",
        "colab_type": "text"
      },
      "source": [
        "## Not a great result!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScHbI0gMF_UD",
        "colab_type": "text"
      },
      "source": [
        "# Logistical regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS16Fk2BFweb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(random_state=0).fit(X, y)\n",
        "y_pred = lr.predict(X_test_scaled)\n",
        "\n",
        "# Confusion Matrix\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCvHwqQcGhgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-U7dCJUGhBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Does scaling change the result?\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(pd.get_dummies(train_X[[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"FamilySize\"]],drop_first=True)) \n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "lr = LogisticRegression(random_state=0).fit(X, y)\n",
        "y_pred = lr.predict(X_test_scaled)\n",
        "\n",
        "# Confusion Matrix\n",
        "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tekBIX0sH2Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRlgrP7pqUMC",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network using Tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztCkMO7zsk1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need to scale our data - using a Standard Scaler rather than MinMax (why?)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Do a new split of our training data\n",
        "X = pd.get_dummies(df[[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"FamilySize\"]],drop_first=True)\n",
        "y = df['Survived'] \n",
        "\n",
        "# Convert y to a categorical value\n",
        "#from keras.utils import to_categorical\n",
        "#y = to_categorical(dy, num_classes=2)\n",
        "\n",
        "\n",
        "# We can create all our splits in one statement\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Fit our scaler to the OneHot encoded data and transform it\n",
        "# Usually 2 ways with all sklearn libraries - a separate fit and transform:\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train) \n",
        "# Or fit and transform in one fit_transform step\n",
        "#X_train_scaled = scaler.fit_transform(X_train) \n",
        "\n",
        "# Now, tranform the test data with the same scaler - note, no \"fit\"!\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67L6G1iMqCzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit a Deep Learning model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(5, input_shape=(5,), activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Split out a validation set to use\n",
        "x_trn, x_val, y_trn, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, shuffle= True)\n",
        "\n",
        "hist = model.fit(x_trn, y_trn, epochs=5, batch_size=5, shuffle=True, validation_data=(x_val, y_val))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5XSmYzkeyPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}